{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of machine learning task, we will not know the true objective function but rather we tend to find the hypothesis function that generalise well our dataset. Therfore, in this notebook, we will look at how to use Genetic Algorithm to optimise parameters for Random Forest. \n",
    "\n",
    "The dataset that we will use for this task is the data that are cllloected from users that interact with mobile ads. The objective for this assignment is to find the probability of mobile users cliking the ads which is used for bid optimosation. Bid optimosation is a very big topic for mobile app because we want to show ads to mobiles users at the right time and right content. So, they would eventually participate in buying products or useing services. As a result, we need to find who we would push the ads to at what time and who are our potential customers for our product based upon the context that they routinely in in order to improve the conversion rate, click through rate etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us import all the library that we will use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read \"train data\" in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start. We Then load training data and label for training data into our work space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"train.csv\",dtype={'id':str, 'ad_format': str, 'ad_format_id': str, 'advertiser_id': str, 'age_rating': str, 'app_age_rating': str, 'app_category_id': str, 'app_company': str, 'app_genre_id': str, 'app_id': str, 'app_quality': str, 'brand_vertical_id': str, 'campaign_id': str, 'country_id': str, 'creative_id': str, 'creative_type': str, 'make': str, 'device_model': str, 'device_os_ver': str, 'device_wh': str, 'device_ww': str, 'genre_id': str, 'hour_local': str, 'isp_type': str, 'language': str, 'line_item_id': str, 'orientation': str, 'platform_id': str, 'publisher': str, 'sdepth': str, 'viewer_token': str, 'ed': str, 'edc': str, 'campaign_type': str, 'city': str, 'isp_name': str, 'ssp': str})\n",
    "y_train = pd.read_csv(\"train.labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then inspect what columns our dataset has including the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'ad_format' 'ad_format_id' 'advertiser_id' 'age_rating'\n",
      " 'app_age_rating' 'app_category_id' 'app_company' 'app_genre_id' 'app_id'\n",
      " 'app_quality' 'brand_vertical_id' 'campaign_id' 'country_id' 'creative_id'\n",
      " 'creative_type' 'make' 'device_model' 'device_os_ver' 'device_wh'\n",
      " 'device_ww' 'genre_id' 'hour_local' 'isp_type' 'language' 'line_item_id'\n",
      " 'orientation' 'platform_id' 'publisher' 'sdepth' 'viewer_token' 'ed' 'edc'\n",
      " 'campaign_type' 'city' 'isp_name' 'ssp']\n",
      "['id' 'label']\n"
     ]
    }
   ],
   "source": [
    "#list column name for training and test set\n",
    "print(X_train.columns.values)\n",
    "print(y_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read \"test\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\",dtype={'id': str, 'ad_format': str, 'ad_format_id': str, 'advertiser_id': str, 'age_rating': str, 'app_age_rating': str, 'app_category_id': str, 'app_company': str, 'app_genre_id': str, 'app_id': str, 'app_quality': str, 'brand_vertical_id': str, 'campaign_id': str, 'country_id': str, 'creative_id': str, 'creative_type': str, 'make': str, 'device_model': str, 'device_os_ver': str, 'device_wh': str, 'device_ww': str, 'genre_id': str, 'hour_local': str, 'isp_type': str, 'language': str, 'line_item_id': str, 'orientation': str, 'platform_id': str, 'publisher': str, 'sdepth': str, 'viewer_token': str, 'ed': str, 'edc': str, 'campaign_type': str, 'city': str, 'isp_name': str, 'ssp': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add logical \"train\" column to train dataset as well as test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add train dataset to training set and test set. This is because later we will merge them together and encode data from string to number in order for RF to calculate the information gain of the classifier. This is to ensure consistancy of data encoding when we make peodiction from classifier that we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[\"train\"] = True\n",
    "test[\"train\"] = False\n",
    "idx =  test[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready we will merge training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201103, 38)\n",
      "(201347, 38)\n",
      "['id' 'ad_format' 'ad_format_id' 'advertiser_id' 'age_rating'\n",
      " 'app_age_rating' 'app_category_id' 'app_company' 'app_genre_id' 'app_id'\n",
      " 'app_quality' 'brand_vertical_id' 'campaign_id' 'country_id' 'creative_id'\n",
      " 'creative_type' 'make' 'device_model' 'device_os_ver' 'device_wh'\n",
      " 'device_ww' 'genre_id' 'hour_local' 'isp_type' 'language' 'line_item_id'\n",
      " 'orientation' 'platform_id' 'publisher' 'sdepth' 'viewer_token' 'ed' 'edc'\n",
      " 'campaign_type' 'city' 'isp_name' 'ssp' 'train']\n",
      "['id' 'ad_format' 'ad_format_id' 'advertiser_id' 'age_rating'\n",
      " 'app_age_rating' 'app_category_id' 'app_company' 'app_genre_id' 'app_id'\n",
      " 'app_quality' 'brand_vertical_id' 'campaign_id' 'country_id' 'creative_id'\n",
      " 'creative_type' 'make' 'device_model' 'device_os_ver' 'device_wh'\n",
      " 'device_ww' 'genre_id' 'hour_local' 'isp_type' 'language' 'line_item_id'\n",
      " 'orientation' 'platform_id' 'publisher' 'sdepth' 'viewer_token' 'ed' 'edc'\n",
      " 'campaign_type' 'city' 'isp_name' 'ssp' 'train']\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(test.shape)\n",
    "print(X_train.columns.values)\n",
    "print(test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concat X_train and X_test\n",
    "frames = [X_train, test]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402450\n",
      "402450\n"
     ]
    }
   ],
   "source": [
    "#check no. before and after concat\n",
    "print(X_train.shape[0]+test.shape[0])\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete empty column [\"app_quality\",\"device_wh\",\"device_ww\",\"ed\",\"edc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data visualisation process, we found that there are some columns that are all empty or has only one value. We therefore will remove them because they will not provide information to split up nodes in the trees. This will also reduce the workload that the algorithm will have to do too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'ad_format' 'ad_format_id' 'advertiser_id' 'age_rating'\n",
      " 'app_age_rating' 'app_category_id' 'app_company' 'app_genre_id' 'app_id'\n",
      " 'brand_vertical_id' 'campaign_id' 'country_id' 'creative_id'\n",
      " 'creative_type' 'make' 'device_model' 'device_os_ver' 'genre_id'\n",
      " 'hour_local' 'isp_type' 'language' 'line_item_id' 'orientation'\n",
      " 'platform_id' 'publisher' 'sdepth' 'viewer_token' 'campaign_type' 'city'\n",
      " 'isp_name' 'ssp' 'train']\n",
      "(402450, 33)\n"
     ]
    }
   ],
   "source": [
    "data.drop([\"app_quality\",\"device_wh\",\"device_ww\",\"ed\",\"edc\"],inplace=True,axis=1)\n",
    "print(data.columns.values)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode categorical data to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to encode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ad_format</th>\n",
       "      <th>ad_format_id</th>\n",
       "      <th>advertiser_id</th>\n",
       "      <th>age_rating</th>\n",
       "      <th>app_age_rating</th>\n",
       "      <th>app_category_id</th>\n",
       "      <th>app_company</th>\n",
       "      <th>app_genre_id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation</th>\n",
       "      <th>platform_id</th>\n",
       "      <th>publisher</th>\n",
       "      <th>sdepth</th>\n",
       "      <th>viewer_token</th>\n",
       "      <th>campaign_type</th>\n",
       "      <th>city</th>\n",
       "      <th>isp_name</th>\n",
       "      <th>ssp</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>440</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>694</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>147291</td>\n",
       "      <td>2</td>\n",
       "      <td>4024</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111</td>\n",
       "      <td>3</td>\n",
       "      <td>440</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>694</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>916</td>\n",
       "      <td>59410</td>\n",
       "      <td>2</td>\n",
       "      <td>7533</td>\n",
       "      <td>2398</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222</td>\n",
       "      <td>3</td>\n",
       "      <td>440</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>694</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>916</td>\n",
       "      <td>50831</td>\n",
       "      <td>2</td>\n",
       "      <td>1197</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333</td>\n",
       "      <td>3</td>\n",
       "      <td>446</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>916</td>\n",
       "      <td>248296</td>\n",
       "      <td>2</td>\n",
       "      <td>10466</td>\n",
       "      <td>2398</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346895</td>\n",
       "      <td>3</td>\n",
       "      <td>446</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>2066</td>\n",
       "      <td>67715</td>\n",
       "      <td>2</td>\n",
       "      <td>1003</td>\n",
       "      <td>2974</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  ad_format  ad_format_id  advertiser_id  age_rating  app_age_rating  \\\n",
       "0       0          3           440              5           3               0   \n",
       "1  111111          3           440              5           3               0   \n",
       "2  222222          3           440              5           3               0   \n",
       "3  333333          3           446              5           3               0   \n",
       "4  346895          3           446              5           3               0   \n",
       "\n",
       "   app_category_id  app_company  app_genre_id  app_id  ...    orientation  \\\n",
       "0               20           51             0     694  ...              1   \n",
       "1               20           51             0     694  ...              1   \n",
       "2               20           51             0     694  ...              1   \n",
       "3               19           41             0       8  ...              3   \n",
       "4               19           41             0       8  ...              3   \n",
       "\n",
       "   platform_id  publisher  sdepth  viewer_token  campaign_type   city  \\\n",
       "0            1         51       2        147291              2   4024   \n",
       "1            1         51     916         59410              2   7533   \n",
       "2            1         51     916         50831              2   1197   \n",
       "3            1         41     916        248296              2  10466   \n",
       "4            1         41    2066         67715              2   1003   \n",
       "\n",
       "   isp_name  ssp  train  \n",
       "0       359    0      1  \n",
       "1      2398    0      1  \n",
       "2       363    0      1  \n",
       "3      2398    0      1  \n",
       "4      2974    0      1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEncoded = data.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "dataEncoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data back to \"training set\" and \"test set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we decoded our dataset, we now will split data back into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = dataEncoded[dataEncoded[\"train\"]==1]\n",
    "test =  dataEncoded[dataEncoded[\"train\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete id column in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will delete \"id\" column from train set which occur during the merging step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad_format' 'ad_format_id' 'advertiser_id' 'age_rating' 'app_age_rating'\n",
      " 'app_category_id' 'app_company' 'app_genre_id' 'app_id'\n",
      " 'brand_vertical_id' 'campaign_id' 'country_id' 'creative_id'\n",
      " 'creative_type' 'make' 'device_model' 'device_os_ver' 'genre_id'\n",
      " 'hour_local' 'isp_type' 'language' 'line_item_id' 'orientation'\n",
      " 'platform_id' 'publisher' 'sdepth' 'viewer_token' 'campaign_type' 'city'\n",
      " 'isp_name' 'ssp' 'train']\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "#remove 'íd' field\n",
    "X_train = X_train.iloc[:,1:]\n",
    "y_train = y_train.iloc[:,1:]\n",
    "#list column name for training and test set\n",
    "print(X_train.columns.values)\n",
    "print(y_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further split \"training data\" to \"train\", \"validation\" and \"test\" set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our current traing set, we would spit it into 3 more chunk\n",
    "* the first chunk will be our actual \"training set\"\n",
    "* the second chunk will be used to evaluate the perfromnace of our classifier whether it performs better than our existing population.\n",
    "* the third chunk will be used for final evaluation of the parameter after the algorithm finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #set seed\n",
    "#random.seed(9001)\n",
    "#split data to train and test\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_train, y_train, test_size=0.25)\n",
    "def getTrainData():\n",
    "    #split train one more time to train and validation\n",
    "    #X_train, X_valid, y_train, y_valid \n",
    "    return cross_validation.train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(150827, 32), (150827, 1)]\n",
      "[(50276, 32), (50276, 1)]\n"
     ]
    }
   ],
   "source": [
    "#check size of training, validation and test dataset\n",
    "print([X_train.shape, y_train.shape])\n",
    "print([X_test.shape, y_test.shape])\n",
    "#print([X_valid.shape, y_valid.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters' values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After everything is set, now let's create a function to make the DNA string. In other words, this function will generate parameter for RF at random to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateParam(trees):\n",
    "    #The number of trees in the forest.\n",
    "    n_estimators = random.randint(50, trees)\n",
    "    #The function to measure the quality of a split.\n",
    "    criterion = random.choice([\"gini\", \"entropy\"])\n",
    "    #The number of features to consider when looking for the best split:\n",
    "    max_features = random.choice([\"auto\", \"sqrt\", \"log2\", None])\n",
    "    #The maximum depth of the tree\n",
    "    max_depth = None if random.uniform(0, 1) < 0.3 else random.randint(5, 300)\n",
    "    #The minimum number of samples required to split an internal node.\n",
    "    min_samples_split = random.randint(10, 100)\n",
    "    #The minimum number of samples in newly created leaves.\n",
    "    min_samples_leaf = random.randint(10, 50)\n",
    "    #The minimum weighted fraction of the input samples required to be at a leaf node.\n",
    "    min_weight_fraction_leaf = random.uniform(0, 0.5)\n",
    "    #Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    max_leaf_nodes = None if random.uniform(0, 1) < 0.1 else random.randint(2, 1000)\n",
    "    #Whether bootstrap samples are used when building trees.\n",
    "    bootstrap = True\n",
    "    #Whether to use out-of-bag samples to estimate the generalization error.\n",
    "    oob_score = random.choice([True, False])\n",
    "    #The number of jobs to run in parallel for both fit and predict.\n",
    "    n_jobs = random.choice([1,2])\n",
    "    #the state of the random number generator\n",
    "    #random_state = random.choice([\"RandomState\", None])\n",
    "    #Controls the verbosity of the tree building process.\n",
    "    #verbose =\n",
    "    #When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.\n",
    "    #otherwise, just fit a whole new forest.\n",
    "    #warm_start = random.choice([True, False])\n",
    "    #Weights associated with classes\n",
    "    #class_weight = random.choice([\"balanced\", \"balanced_subsample\",None])\n",
    "    return [n_estimators, criterion, max_features, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, bootstrap, oob_score, n_jobs]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print generateParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"LogLoss\" function that use to eveluate how well our classifier is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EvalLogLoss(predicted, actual):\n",
    "    return sklearn.metrics.log_loss(actual, predicted, eps=1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get first generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to create our first population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFistGen(n, trees):\n",
    "    pop = list()\n",
    "    #n = 2 #50\n",
    "    for i in range(0,n):\n",
    "        pop.extend([generateParam(trees)])\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computeProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to eveluate parameter that we randomly generate from GA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeProb(dna,X_train,y_train,X_valid):\n",
    "    clf =  RandomForestClassifier(n_estimators=dna[0], criterion=dna[1], max_features=dna[2], \\\n",
    "                              max_depth=dna[3], min_samples_split=dna[4], min_samples_leaf=dna[5], \\\n",
    "                              min_weight_fraction_leaf=dna[6], max_leaf_nodes=dna[7], bootstrap=dna[8], \\\n",
    "                              oob_score=dna[9], n_jobs=dna[10])\n",
    "    #clf =  RandomForestClassifier(n_estimators=dna[0])\n",
    "    clf.fit(X_train.as_matrix(), y_train.as_matrix())\n",
    "    predicted = pd.DataFrame(clf.predict_proba(X_valid))\n",
    "    return predicted[0].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalFirstGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for eveluate our population in first generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeFirstGen(pop):\n",
    "    loss_lst = list()\n",
    "    for dna in pop:\n",
    "        X_train, X_valid, y_train, y_valid = getTrainData()\n",
    "        predicted = computeProb(dna, X_train, y_train, X_valid)\n",
    "        #loss = EvalLogLoss(predicted[0].as_matrix(), y_valid)\n",
    "        loss = EvalLogLoss(predicted, y_valid)\n",
    "        loss_lst.extend([loss])\n",
    "    return loss_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *run RF*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform GA search through our parameter space and try to find to optimal one and write into csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start RF\n",
      "start third.csv\n"
     ]
    }
   ],
   "source": [
    "def runRF(numberOfPop, numberOfTrees, numberOfGeneration, fileNames):\n",
    "    print \"start RF\"\n",
    "    for f in fileNames:\n",
    "        print \"start \"+f\n",
    "        pop = getFistGen(numberOfPop, numberOfTrees)\n",
    "        popLoss = computeFirstGen(pop)\n",
    "        for i in range(0,numberOfGeneration):\n",
    "            #create training dataset and validation dataset\n",
    "            X_train, X_valid, y_train, y_valid = getTrainData()\n",
    "            #hybridge and produce offspring\n",
    "            parent1 = random.choice(pop)\n",
    "            parent2 = random.choice(pop)\n",
    "            pos = random.randint(1, len(parent1)-1)\n",
    "            offspring1 = parent1[0:pos]+parent2[pos:]\n",
    "            offspring2 = parent2[0:pos]+parent1[pos:]\n",
    "            #mutate offspring \n",
    "            newParam = generateParam(numberOfTrees)\n",
    "            pos = random.randint(0, len(pop[0])-1)\n",
    "            offspring1[pos] = newParam[pos]\n",
    "            pos = random.randint(0, len(pop[0])-1)\n",
    "            offspring2[pos] = newParam[pos]\n",
    "            #evaluate offsprings\n",
    "            offspring1Predicted = computeProb(offspring1,X_train,y_train,X_valid)\n",
    "            offspring1Loss = EvalLogLoss(offspring1Predicted, y_valid)\n",
    "            offspring2Predicted = computeProb(offspring2,X_train,y_train,X_valid)\n",
    "            offspring2Loss = EvalLogLoss(offspring2Predicted, y_valid)\n",
    "            #print(offspring2Loss)\n",
    "            #add offsprings to pop\n",
    "            pop.extend([offspring1,offspring2])\n",
    "            popLoss.extend([offspring1Loss,offspring2Loss])\n",
    "            mapPopToLoss = zip(popLoss,pop)\n",
    "            sortedMapPopToLoss = sorted(mapPopToLoss)\n",
    "            newPopLoss = [key[0] for key in sortedMapPopToLoss]\n",
    "            newPop = [value[1] for value in sortedMapPopToLoss]\n",
    "            pop = pop[0:numberOfPop]\n",
    "            popLoss = popLoss[0:numberOfPop]\n",
    "            if i%25==0:\n",
    "                print i\n",
    "                print(len(sortedMapPopToLoss))\n",
    "        df = pd.DataFrame(sorted(zip(popLoss,pop)))\n",
    "        df.to_csv(f)\n",
    "    return \"finish RF\"\n",
    "\n",
    "#runRF(numberOfPop, numberOfTrees, numberOfGeneration, fileNames)\n",
    "print runRF(50, 5000, 100, [\"third.csv\",\"fourth.csv\",\"fifth.csv\",\"sixth.csv\",\"seventh.csv\",\"eight.csv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test params on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we sucessfully run GA+RF, let's inspect our results. We will use the DNA string that provides the best logLoss on our third chunk (\"test set\") that we left out previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.770681</td>\n",
       "      <td>[786, 'entropy', 'sqrt', 61, 109, 487, 0.48675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.800684</td>\n",
       "      <td>[373, 'gini', 'log2', 25, 144, 355, 0.48044017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.806080</td>\n",
       "      <td>[975, 'entropy', 'sqrt', None, 76, 965, 0.4468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.809603</td>\n",
       "      <td>[272, 'gini', 'auto', 5, 156, 619, 0.443056808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.833851</td>\n",
       "      <td>[148, 'entropy', 'sqrt', 224, 208, 705, 0.4058...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0         0                                                  1\n",
       "28          28  1.770681  [786, 'entropy', 'sqrt', 61, 109, 487, 0.48675...\n",
       "40          40  1.800684  [373, 'gini', 'log2', 25, 144, 355, 0.48044017...\n",
       "23          23  1.806080  [975, 'entropy', 'sqrt', None, 76, 965, 0.4468...\n",
       "7            7  1.809603  [272, 'gini', 'auto', 5, 156, 619, 0.443056808...\n",
       "10          10  1.833851  [148, 'entropy', 'sqrt', 224, 208, 705, 0.4058..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstrun = pd.read_csv(\"first_run.csv\")\n",
    "sortfrun = firstrun.sort(columns=[\"0\"])\n",
    "sortfrun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLossOnTest(dna,X_train, y_train, X_test, y_test):\n",
    "    clf =  RandomForestClassifier(n_estimators=dna[0], criterion=dna[1], max_features=dna[2], \\\n",
    "                              max_depth=dna[3], min_samples_split=dna[4], min_samples_leaf=dna[5], \\\n",
    "                              min_weight_fraction_leaf=dna[6], max_leaf_nodes=dna[7], bootstrap=dna[8], \\\n",
    "                              oob_score=dna[9], n_jobs=dna[10])\n",
    "    #clf =  RandomForestClassifier(n_estimators=dna[0])\n",
    "    clf.fit(X_train.as_matrix(), y_train.as_matrix())\n",
    "    predicted = pd.DataFrame(clf.predict_proba(X_test))\n",
    "    predicted = predicted[0].as_matrix()\n",
    "    return EvalLogLoss(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dna = ast.literal_eval(sortfrun[\"1\"][28])\n",
    "print computeLossOnTest(dna,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will generate a submission file that use the best DNA found so far to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write sucessfully\n"
     ]
    }
   ],
   "source": [
    "def makeSubmissionFile(dna,X_train, y_train, test,idx):\n",
    "    test = test.iloc[:,1:]\n",
    "    #test.set_index(keys= [\"id\"], drop=True, inplace=True)\n",
    "    clf =  RandomForestClassifier(n_estimators=dna[0], criterion=dna[1], max_features=dna[2], \\\n",
    "                              max_depth=dna[3], min_samples_split=dna[4], min_samples_leaf=dna[5], \\\n",
    "                              min_weight_fraction_leaf=dna[6], max_leaf_nodes=dna[7], bootstrap=dna[8], \\\n",
    "                              oob_score=dna[9], n_jobs=dna[10])\n",
    "    #clf =  RandomForestClassifier(n_estimators=dna[0])\n",
    "    clf.fit(X_train.as_matrix(), y_train.as_matrix())\n",
    "    submission = pd.DataFrame(clf.predict_proba(test), index=idx, columns=[\"prediction\",\"1_pred\"])\n",
    "    submission = submission[\"prediction\"]\n",
    "    submission.to_csv(\"submission1.csv\",header=True)\n",
    "    return \"write sucessfully\"\n",
    "print makeSubmissionFile(dna,X_train, y_train, test, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## side note and recommodation\n",
    "\n",
    "If you run this on a single machine, the algorithm can be finished on a small number of trees e.g.500 in roughtly one night. Further, with the dataset, I tried I found that there is not much improvement for one GA. Therefore, I recommend that if you want to go with small number of trees, try multiple interations. However, althogh I did not try, you can take an advantage of parallel compotation (say Condors) to distribute your jobs in to cluster. This could help when you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "*last edit 24/10/2016*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
